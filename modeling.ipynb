{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55878781",
   "metadata": {},
   "source": [
    "# 00 Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0282bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782eae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Stopword Removal & Stemming (Bahasa Indonesia)\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import stanza\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Word Embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Modeling\n",
    "from scipy import stats\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics.pairwise import linear_kernel,cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7429dd9",
   "metadata": {},
   "source": [
    "# 01 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18cffb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data scientist, financial conglomerates superv...</td>\n",
       "      <td>['role', 'purpose', 'execute', 'suptech', 'dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>['job', 'description', 'responsibility', 'data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data annotator</td>\n",
       "      <td>['job', 'description', 'key', 'responsibility'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data scientist (artificial intelligence)</td>\n",
       "      <td>['join', 'dcap', 'dynamic', 'fastgrowing', 'te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>['key', 'responsibility', 'data', 'exploration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>['job', 'description', 'work', 'closely', 'int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>data scientist - pricing</td>\n",
       "      <td>['people', 'job', 'description', 'data', 'scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>data analyst</td>\n",
       "      <td>['tugas', 'tanggung', 'jawab', 'mengumpulkan',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>data analyst – pricing staff</td>\n",
       "      <td>['key', 'responsibility', 'collect', 'process'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>data scientist (analytics)</td>\n",
       "      <td>['description', 'grab', 'workplace', 'grab', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    data scientist, financial conglomerates superv...   \n",
       "1                                       data scientist   \n",
       "2                                       data annotator   \n",
       "3             data scientist (artificial intelligence)   \n",
       "4                                       data scientist   \n",
       "..                                                 ...   \n",
       "475                                     data scientist   \n",
       "476                           data scientist - pricing   \n",
       "477                                       data analyst   \n",
       "478                       data analyst – pricing staff   \n",
       "479                         data scientist (analytics)   \n",
       "\n",
       "                                                tokens  \n",
       "0    ['role', 'purpose', 'execute', 'suptech', 'dat...  \n",
       "1    ['job', 'description', 'responsibility', 'data...  \n",
       "2    ['job', 'description', 'key', 'responsibility'...  \n",
       "3    ['join', 'dcap', 'dynamic', 'fastgrowing', 'te...  \n",
       "4    ['key', 'responsibility', 'data', 'exploration...  \n",
       "..                                                 ...  \n",
       "475  ['job', 'description', 'work', 'closely', 'int...  \n",
       "476  ['people', 'job', 'description', 'data', 'scie...  \n",
       "477  ['tugas', 'tanggung', 'jawab', 'mengumpulkan',...  \n",
       "478  ['key', 'responsibility', 'collect', 'process'...  \n",
       "479  ['description', 'grab', 'workplace', 'grab', '...  \n",
       "\n",
       "[480 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_jobstreet.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5507615",
   "metadata": {},
   "source": [
    "# 02 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f83ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Lowercase Function (Multi-column) ---\n",
    "def lowercase_columns(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(str).str.lower()\n",
    "    return df\n",
    "\n",
    "# --- 2. Clean Text Function ---\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[\\n\\r\\t]+\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[,.!?]\", \"\", text)\n",
    "    return np.nan if text == \"\" else text\n",
    "\n",
    "# --- 3. Translate Indonesian Texts to English ---\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except:\n",
    "        return text  # fallback\n",
    "\n",
    "# --- 4. Replace Slang ---\n",
    "df_slang = pd.read_csv(\"slang.csv\")\n",
    "slang_dict = dict(zip(df_slang['slang'], df_slang['formal']))\n",
    "additional_slang = {}  # Tambahkan jika ada\n",
    "slang_dict.update(additional_slang)\n",
    "\n",
    "def replace_slang(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    words = text.split()\n",
    "    return \" \".join([slang_dict.get(w, w) for w in words])\n",
    "\n",
    "# --- 5. Tokenizing ---\n",
    "def tokenizing_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# --- 6. Remove Stopwords ---\n",
    "stopword_manual = pd.read_csv(\"stopword.csv\", header=None)\n",
    "custom_stopwords = set(stopword_manual.iloc[:, 0].str.lower())\n",
    "custom_stopwords.update([])  # Tambahkan manual tambahan jika ada\n",
    "factory_stopword = StopWordRemoverFactory()\n",
    "stopwords_nltk = set(stopwords.words('indonesian'))\n",
    "\n",
    "def remove_manual_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in custom_stopwords]\n",
    "\n",
    "# --- 7. Lemmatization ---\n",
    "def lemmatize_flex(word):\n",
    "    lemma_v = Word(word).lemmatize(\"v\")\n",
    "    return lemma_v if lemma_v != word else Word(word).lemmatize(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9179f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3da602b4",
   "metadata": {},
   "source": [
    "# 03 Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb72ee",
   "metadata": {},
   "source": [
    "## a. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = pd.concat([df[\"tokens\"], df_kunci[\"tokens\"]], ignore_index=True).tolist()\n",
    "\n",
    "# Word2Vec (CBOW default)\n",
    "w2v_model = Word2Vec(sentences=all_sentences, vector_size=100, window=7, min_count=1, workers=4)\n",
    "\n",
    "def sentence_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Hitung vektor kalimat\n",
    "df[\"w2v_vec\"] = df[\"tokens\"].apply(lambda x: sentence_vector(x, w2v_model))\n",
    "df_kunci[\"w2v_vec\"] = df_kunci[\"tokens\"].apply(lambda x: sentence_vector(x, w2v_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df2.loc[0, \"tokens\"]\n",
    "\n",
    "# Matriks vektor Word2Vec untuk token\n",
    "matrix_w2v = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "matrix_w2v_df = pd.DataFrame(matrix_w2v, index=[word for word in tokens if word in w2v_model.wv])\n",
    "\n",
    "print(\"=== Matriks Vektor Word2Vec (baris per kata) ===\")\n",
    "print(matrix_w2v_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee73ad",
   "metadata": {},
   "source": [
    "## b. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(sentences=all_sentences, vector_size=100, window=7, min_count=1, workers=4)ft_model = FastText(sentences=all_sentences, vector_size=100, window=7, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233513eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "df2[\"ft_vec\"] = df2[\"tokens\"].apply(lambda x: sentence_vector(x, ft_model))\n",
    "df2_kunci[\"ft_vec\"] = df2_kunci[\"tokens\"].apply(lambda x: sentence_vector(x, ft_model))\n",
    "\n",
    "matrix_ft = [ft_model.wv[word] for word in tokens if word in ft_model.wv]\n",
    "matrix_ft_df = pd.DataFrame(matrix_ft, index=[word for word in tokens if word in ft_model.wv])\n",
    "print(\"=== Matriks Vektor FastText (baris per kata) ===\")\n",
    "print(matrix_ft_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6da85",
   "metadata": {},
   "source": [
    "# 04 Labelling & Similarity Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fea50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8725fd00",
   "metadata": {},
   "source": [
    "# 05 Recommendation Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0cd07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8b467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2ae21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
